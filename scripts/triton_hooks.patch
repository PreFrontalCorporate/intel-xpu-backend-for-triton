diff --git a/test/inductor/test_profiler.py b/test/inductor/test_profiler.py
index ff557874881..0b0f5cab75b 100644
--- a/test/inductor/test_profiler.py
+++ b/test/inductor/test_profiler.py
@@ -183,7 +183,7 @@ class DynamoProfilerTests(torch._inductor.test_case.TestCase):
         "compile_threads", 1
     )  # This test monkey patches global variables, which workers don't see
     def test_inductor_profiling_triton_hooks(self):
-        from triton.compiler import CompiledKernel  # @manual
+        import triton  # @manual
 
         hooks_called = {"enter": False, "exit": False}
 
@@ -193,8 +193,8 @@ class DynamoProfilerTests(torch._inductor.test_case.TestCase):
         def launch_exit_hook(lazy_dict):
             hooks_called["exit"] = True
 
-        CompiledKernel.launch_enter_hook = launch_enter_hook
-        CompiledKernel.launch_exit_hook = launch_exit_hook
+        triton.config.runtime.launch_enter_hook = launch_enter_hook
+        triton.config.runtime.launch_exit_hook = launch_exit_hook
 
         def fn(x, y):
             return torch._foreach_add(x, y)
diff --git a/torch/_inductor/runtime/static_cuda_launcher.py b/torch/_inductor/runtime/static_cuda_launcher.py
index b5aca5ac03e..7341696ba54 100644
--- a/torch/_inductor/runtime/static_cuda_launcher.py
+++ b/torch/_inductor/runtime/static_cuda_launcher.py
@@ -44,13 +44,23 @@ class StaticallyLaunchedCudaKernel:
         self.declared_constexprs = kernel.src.fn.constexprs
 
         self.hash = kernel.hash
-        if (
-            kernel.__class__.launch_enter_hook is not None
-            or kernel.__class__.launch_exit_hook is not None
-        ):
-            raise NotImplementedError(
-                "We don't support launch enter or launch exit hooks"
-            )
+        if hasattr(kernel.__class__, "launch_enter_hook"):
+            if (
+                kernel.__class__.launch_enter_hook is not None
+                or kernel.__class__.launch_exit_hook is not None
+            ):
+                raise NotImplementedError(
+                    "We don't support launch enter or launch exit hooks"
+                )
+        else:
+            import triton
+            if (
+                triton.config.runtime.launch_enter_hook is not None
+                or triton.config.runtime.launch_exit_hook is not None
+            ):
+                raise NotImplementedError(
+                    "We don't support launch enter or launch exit hooks"
+                )
         self.num_warps = kernel.metadata.num_warps
         self.shared = (
             kernel.shared if hasattr(kernel, "shared") else kernel.metadata.shared
diff --git a/torch/_inductor/runtime/triton_heuristics.py b/torch/_inductor/runtime/triton_heuristics.py
index 93fb36e12bb..bab316130fa 100644
--- a/torch/_inductor/runtime/triton_heuristics.py
+++ b/torch/_inductor/runtime/triton_heuristics.py
@@ -1423,11 +1423,19 @@ class TritonCompileResult(CompileResult[CompiledKernel]):
             binary.shared if hasattr(binary, "shared") else binary.metadata.shared
         )
 
+        if hasattr(binary.__class__, "launch_enter_hook"):
+            launch_enter_hook = binary.__class__.launch_enter_hook
+            launch_exit_hook = binary.__class__.launch_exit_hook
+        else:
+            import triton
+            launch_enter_hook = triton.config.runtime.launch_enter_hook
+            launch_exit_hook = triton.config.runtime.launch_exit_hook
+
         scope = {
             "grid_meta": cfg.kwargs,
             "bin": binary,
-            "launch_enter_hook": binary.__class__.launch_enter_hook,
-            "launch_exit_hook": binary.__class__.launch_exit_hook,
+            "launch_enter_hook": launch_enter_hook,
+            "launch_exit_hook": launch_exit_hook,
             "metadata": (
                 binary.packed_metadata
                 if hasattr(binary, "packed_metadata")
@@ -1478,7 +1486,7 @@ class TritonCompileResult(CompileResult[CompiledKernel]):
             # `launch_enter_hook` is installed.  So if we don't have that hook installed,
             # we want to burn None in to the launch args with zero overhead.
             # See https://github.com/pytorch/pytorch/issues/123597
-            if binary.__class__.launch_enter_hook:
+            if launch_enter_hook:
                 launch_metadata = f"bin.launch_metadata((grid_0, grid_1, grid_2), stream, {', '.join(call_args)})"
             else:
                 launch_metadata = "None"
